{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1016a66",
   "metadata": {},
   "source": [
    "# Softmax Layer Exercise (with Batch Support & Numerical Stability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b845b17-2c13-4c38-8d7f-594888b37cac",
   "metadata": {},
   "source": [
    "**Goal:** Implement a numerically stable softmax function that works for both a single vector of logits and a batch of logits.\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "By completing this exercise, you will:  \n",
    "\n",
    "- **Understand the role of softmax** in neural networks and why it is used to convert raw scores (logits) into probabilities.  \n",
    "- **Implement the softmax function** in Python using NumPy, both for single vectors (1D arrays) and for batches of data (2D arrays).  \n",
    "- **Apply numerical stability techniques** by subtracting the maximum logit before exponentiation to prevent overflow errors.  \n",
    "- **Verify correctness** of your implementation with simple unit tests that check properties like “outputs sum to 1” and “softmax is invariant to constant shifts.”  \n",
    "- **Practice debugging and testing skills** — interpreting failed tests and fixing implementation mistakes.  \n",
    "- Build confidence in working with **array operations, broadcasting, and vectorization** in NumPy.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb199e-d843-47f7-8b49-6c0c4cb4aa4a",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "In classification tasks, a neural network outputs raw scores called **logits**.  \n",
    "These values are not probabilities — they can be any real numbers.  \n",
    "\n",
    "The **softmax function** transforms logits into probabilities:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}\n",
    "$$\n",
    "\n",
    "Key properties:\n",
    "- All outputs are **between 0 and 1**.\n",
    "- The outputs **sum to 1**, so they form a valid probability distribution.\n",
    "- The largest logit corresponds to the class with the highest probability.\n",
    "\n",
    "👉 **Numerical stability trick**:  \n",
    "If logits are large, exponentials can overflow.  \n",
    "To prevent this, we subtract the maximum value before exponentiation:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z)_i = \\frac{e^{\\,z_i - \\max(z)}}{\\sum_{j=1}^{C} e^{\\,z_j - \\max(z)}}\n",
    "$$\n",
    "\n",
    "This subtraction does **not** change the result — it only keeps numbers manageable for the computer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c0023-b7d0-49a5-9964-33a774c057f4",
   "metadata": {},
   "source": [
    "## Your Task\n",
    "\n",
    "Implement the function `softmax(x)`.\n",
    "\n",
    "Requirements:\n",
    "- If `x` is a **1D NumPy array** of shape `(C,)`, return a vector of shape `(C,)`.\n",
    "- If `x` is a **2D NumPy array** of shape `(N, C)`, return a matrix of shape `(N, C)`.  \n",
    "  Each row should be a probability distribution that sums to 1.\n",
    "- Use the **numerical stability trick** (subtract the max per vector or per row).\n",
    "- **Do not modify the input array in place.**\n",
    "\n",
    "Hints:\n",
    "- Use `x.ndim` to check if the input is 1D or 2D.\n",
    "- Use `keepdims=True` when subtracting or summing across rows, so broadcasting works correctly.\n",
    "- Test your function on both **vector** (1D) and **batch** (2D) cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365334f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute numerically stable softmax\"\"\"\n",
    "    # your code here\n",
    "    raise NotImplementedError(\"Implement the softmax function\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dabefa",
   "metadata": {},
   "source": [
    " ## Unit tests and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfddaa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   # <-- add this line\n",
    "\n",
    "vec = np.array([2.0, 1.0, 0.0])\n",
    "out = softmax(vec)\n",
    "assert out.shape == (3,)\n",
    "assert abs(out.sum() - 1) < 1e-9\n",
    "assert np.all(out >= 0), \"Probabilities must be non-negative\"\n",
    "print(\"Vector test passed\")\n",
    "\n",
    "batch = np.array([[1, 2, 3],\n",
    "                  [0, 0, 0]])\n",
    "outb = softmax(batch)\n",
    "assert outb.shape == batch.shape\n",
    "assert np.allclose(outb.sum(axis=1), np.ones(2))\n",
    "print(\"Batch test passed\")\n",
    "\n",
    "print(\"All tests passed ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb3f2d-458d-4db6-8b30-a8f5215e2f32",
   "metadata": {},
   "source": [
    "### Try it yourself\n",
    "\n",
    "Run the cell below to validate your implementation. It checks:\n",
    "- Vector case: shape, sum to 1, and reasonable values.\n",
    "- Batch case: per-row probabilities sum to 1.\n",
    "- (Optional) You can add more tests of your own.\n",
    "\n",
    "If a test fails, read the error message, fix your implementation, and re-run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf0d313-3ca4-4582-8f3e-5cb752d2e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it (feel free to change numbers)\n",
    "logits = np.array([[2.0, 1.0, 0.0],\n",
    "                   [5.0, -1.0, 1.0]])\n",
    "probs = softmax(logits)\n",
    "probs, probs.sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54ece3b",
   "metadata": {},
   "source": [
    "## Instructor's Solution\n",
    "\n",
    "> This cell would be hidden from learners in a real course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c21063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    import numpy as np\n",
    "    if x.ndim == 1:\n",
    "        shifted = x - np.max(x)\n",
    "        exps = np.exp(shifted)\n",
    "        return exps/np.sum(exps)\n",
    "    elif x.ndim == 2:\n",
    "        shifted = x - np.max(x, axis=1, keepdims=True)\n",
    "        exps = np.exp(shifted)\n",
    "        return exps/np.sum(exps, axis=1, keepdims=True)\n",
    "    else:\n",
    "        raise ValueError(\"Input must be 1D or 2D\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
